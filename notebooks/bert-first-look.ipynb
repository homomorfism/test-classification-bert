{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "0\n",
      "7592\n",
      "2088\n"
     ]
    }
   ],
   "source": [
    "# The vocab is an ordered dictionary - key/value pairs.\n",
    "# This is how to see which tokens are associated with a particular word.\n",
    "\n",
    "bert_vocab = tokenizer.vocab\n",
    "\n",
    "print(bert_vocab['[CLS]'])\n",
    "print(bert_vocab['[SEP]'])\n",
    "print(bert_vocab['[PAD]'])\n",
    "\n",
    "print(bert_vocab['hello'])\n",
    "print(bert_vocab['world'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 7592, 2045, 1012,  102,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Displaying format of inputs to model\n",
    "\n",
    "MAX_LEN = 10  # This value could be set as 256, 512 etc.\n",
    "\n",
    "sentence1 = 'Hello there.'\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    sentence1,  # Sentence to encode.\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=MAX_LEN,  # Pad or truncate.\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',  # Return pytorch tensors.\n",
    ")\n",
    "\n",
    "encoded_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 7592, 2045, 1012,  102,    0,    0,    0,    0,    0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_dict['input_ids'][0]\n",
    "token_type_ids = encoded_dict['token_type_ids'][0]\n",
    "att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "# These are torch tensors.\n",
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(att_mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] hello there. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "hello there.\n"
     ]
    }
   ],
   "source": [
    "## Decoding a sequence of tokens\n",
    "\n",
    "# skip_special_tokens â€“ if this is set to True, then special tokens will be replaced.\n",
    "\n",
    "# Note that do_lower_case=True in the tokenizer.\n",
    "# This is why all text is lower case.\n",
    "\n",
    "a = tokenizer.decode(input_ids,\n",
    "                     skip_special_tokens=False)\n",
    "\n",
    "b = tokenizer.decode(input_ids,\n",
    "                     skip_special_tokens=True)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(MODEL_TYPE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "250002"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%# Check the vocab size\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bos_token': '<s>',\n 'eos_token': '</s>',\n 'unk_token': '<unk>',\n 'sep_token': '</s>',\n 'pad_token': '<pad>',\n 'cls_token': '<s>',\n 'mask_token': '<mask>'}"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35378,  2685,     5,     2,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Hello there.'\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    sentence1,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'  # return pytorch tensors\n",
    ")\n",
    "\n",
    "print(encoded_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%MAX_LEN = 10 # This value could be set as 256, 512 etc.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 35378,  2685,     5,     2,     1,     1,     1,     1,     1])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_dict['input_ids'][0]\n",
    "att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "print(input_ids)\n",
    "print(att_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%# These have already been converted to torch tensors.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35378,  2685,     5,     2,     2, 11249,   621,   398,    32,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Hello there.'\n",
    "sentence2 = 'How are you?'\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    sentence1, sentence2,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'  # return pytorch tensors\n",
    ")\n",
    "\n",
    "print(encoded_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%MAX_LEN = 15\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 35378,  2685,     5,     2,     2, 11249,   621,   398,    32,\n",
      "            2])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_dict['input_ids'][0]\n",
    "att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "# These are torch tensors.\n",
    "print(input_ids)\n",
    "print(att_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%input_ids = encoded_dict['input_ids'][0]\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Hello there.</s></s> How are you?</s>\n",
      "Hello there. How are you?\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer.decode(input_ids,\n",
    "                     skip_special_tokens=False)\n",
    "\n",
    "b = tokenizer.decode(input_ids,\n",
    "                     skip_special_tokens=True)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 35378, 2685, 5, 11249, 621, 398, 32, 31901, 10, 26267, 5155, 5, 3293, 2], [0, 5155, 5, 3293, 83, 10, 3034, 32, 2, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], 'overflow_to_sample_mapping': [0, 0]}\n",
      "<s> Hello there. How are you? Have a nice day. This</s>\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 15  # This value could be set as 256, 512 etc.\n",
    "\n",
    "sentence1 = 'Hello there. How are you? Have a nice day. This is a test?'\n",
    "\n",
    "encoded_dict = tokenizer(\n",
    "    sentence1,\n",
    "    max_length=MAX_LEN,\n",
    "    stride=3,\n",
    "    pad_to_max_length=True,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "print(encoded_dict)\n",
    "# print(encoded_dict['input_ids'])\n",
    "# print(encoded_dict['overflowing_tokens'])\n",
    "\n",
    "decoded_line = tokenizer.decode(\n",
    "    token_ids=encoded_dict['input_ids'][0],\n",
    ")\n",
    "print(decoded_line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%## Example with overflowing tokens and Stride\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}